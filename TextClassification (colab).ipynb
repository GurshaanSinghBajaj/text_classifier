{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXi91WaTNtTh"
   },
   "outputs": [],
   "source": [
    "# All the import statements\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "import urllib.request\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import tarfile\n",
    "from string import punctuation\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9vFfIqn1NtTn",
    "outputId": "78603cd3-5351-422d-ccdf-561759264970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# Declaring various classes of the newsgroups\n",
    "\n",
    "a=[] ## list decleration for file names\n",
    "\n",
    "classes=[\"alt.atheism\",\"comp.graphics\",\"comp.os.ms-windows.misc\",\"comp.sys.ibm.pc.hardware\",\n",
    "         \"comp.sys.mac.hardware\",\"comp.windows.x\",\"misc.forsale\",\"rec.autos\",\"rec.motorcycles\",\n",
    "         \"rec.sport.baseball\",\"rec.sport.hockey\",\"sci.crypt\",\"sci.electronics\",\"sci.med\",\n",
    "         \"sci.space\",\"soc.religion.christian\",\"talk.politics.guns\",\"talk.politics.mideast\",\n",
    "         \"talk.politics.misc\",\"talk.religion.misc\"]\n",
    "\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "Bz78o_qLNtTs",
    "outputId": "4386b351-7e74-4c5b-b9cf-364b63a41de3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gurshaan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "{'from', \"hadn't\", 'only', 'of', 'or', 'you', 't', 'they', 'she', \"shan't\", 'shouldn', 'the', 'your', 'ain', 'these', 'most', \"don't\", 'her', 'against', 'themselves', 'mustn', 'a', \"wasn't\", 'have', \"won't\", 'on', 'under', 'not', \"isn't\", 'those', 'itself', \"you'd\", \"should've\", 'wouldn', 'am', 'how', \"weren't\", 'being', 'and', 'above', 'him', 've', 'i', 'doing', 'because', 'y', 'when', \"that'll\", 'm', 'it', 'having', 'at', 'such', 'didn', 'whom', 'ma', 'in', 'off', 'again', 'some', 'are', 'few', \"doesn't\", 'he', 'into', 'through', \"shouldn't\", 're', 'can', 'over', 'same', 'ourselves', 'during', 'each', 'as', 'both', 'their', 'just', 'does', 'his', 'be', 'down', \"wouldn't\", 'were', 'too', 'while', 'needn', 'where', 'before', 'my', 'should', 'than', 'don', 'wasn', 'any', 'what', 'no', 'is', 'do', 'himself', 'has', 'been', 'had', 'own', 'between', 'yourselves', 'there', 'was', 'who', 'then', 's', 'about', 'myself', \"she's\", 'them', 'this', 'more', 'its', 'an', 'ours', 'couldn', 'all', 'd', 'doesn', 'that', 'further', 'below', 'hers', \"you'll\", \"you're\", 'once', 'isn', 'weren', 'won', 'our', 'nor', 'why', 'll', 'after', 'shan', 'with', 'out', 'other', 'which', \"you've\", 'until', 'yours', 'we', \"mightn't\", 'me', 'did', 'if', 'aren', 'haven', \"needn't\", \"aren't\", \"didn't\", 'herself', 'hasn', 'here', 'o', \"haven't\", \"mustn't\", 'hadn', \"couldn't\", 'yourself', 'to', 'up', 'mightn', \"it's\", 'will', 'now', 'very', 'theirs', 'but', 'for', 'so', 'by', \"hasn't\"}\n"
     ]
    }
   ],
   "source": [
    "## Extracting the stopwords from the nltk library\n",
    "nltk.download('stopwords')\n",
    "StopWords=set(stopwords.words('english'))\n",
    "print(StopWords)\n",
    "\n",
    "# Our own list of some block words to be avoided; observed from the documents\n",
    "\n",
    "block_words = ['newsgroups', 'xref', 'path', 'from', 'subject', 'sender', 'organisation', 'apr',\n",
    "               'gmt', 'last','better','never','every','even','two','good','used','first','need',\n",
    "               'going','must','really','might','well','without','made','give','look','try','far',\n",
    "               'less','seem','new','make','many','way','since','using','take','help','thanks','send',\n",
    "               'free','may','see','much','want','find','would','one','like','get','use','also','could',\n",
    "               'say','us','go','please','said','set','got','sure','come','lot','seems','able','anything',\n",
    "               'put', '--', '|>', '>>', '93', 'xref', 'cantaloupe.srv.cs.cmu.edu', '20', '16', \n",
    "               \"max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'\", '21', '19', '10', \n",
    "               '17', '24', 'reply-to:', 'thu', 'nntp-posting-host:', 're:','25''18'\"i'd\"'>i''22''fri,''23''>the',\n",
    "               'references:','xref:','sender:','writes:','1993','organization:']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "a9os1ahLNtTv",
    "outputId": "ba564a50-bf92-4cd3-dcbf-86d18709ca79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'>', '%', '|', '@', '+', '&', '$', '<', '/', '^', '~', '{', '!', '`', ',', '(', '*', ')', \"'\", '.', '_', ';', '#', '\"', '[', '}', ':', '=', ']', '\\\\', '-', '?'}\n"
     ]
    }
   ],
   "source": [
    "# declaring a set of special characters and numbers\n",
    "punc = (set(punctuation))\n",
    "print (punc)\n",
    "num = {'0','1','2','3','4','5','6','7','8','9'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dgwb5ar_NtTy"
   },
   "outputs": [],
   "source": [
    "data={}\n",
    "data[\"train\"]={}\n",
    "data[\"test\"]={}\n",
    "for i in range(20):\n",
    "    s=classes[i]\n",
    "    data[s]=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "QVDn1BJDOU3f",
    "outputId": "ac4bb549-a922-453b-8d72-efd4f7325aec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a.tar.gz', <http.client.HTTPMessage at 0x14313b3d748>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## statement to retrieve the dataset from the link\n",
    "\n",
    "urllib.request.urlretrieve (\"https://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/20_newsgroups.tar.gz\", \"a.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tS74QwacOWag"
   },
   "outputs": [],
   "source": [
    "## Statements to extract the database documents from web\n",
    "\n",
    "tar = tarfile.open(\"a.tar.gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8e9w6OiVNtT1"
   },
   "outputs": [],
   "source": [
    "## adding all the file names to a list\n",
    "\n",
    "for i in range(20):\n",
    "    a.clear()\n",
    "    for files in os.listdir(\"./20_newsgroups/\"+classes[i]):\n",
    "        data[classes[i]].append(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XdTRnX4NtT3"
   },
   "outputs": [],
   "source": [
    "# temporary list declaration\n",
    "\n",
    "word_present={}\n",
    "alternate_word_array=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jftfRzxONtT5"
   },
   "outputs": [],
   "source": [
    "## This piece of code will extract the words from the documents and calculate\n",
    "## their occurences in a dictionary\n",
    "\n",
    "for i in range(20): ## number of newsgroups\n",
    "    for j in range(len(data[classes[i]])):\n",
    "        \n",
    "        ## declaring the path of every individual file\n",
    "        path = \"./20_newsgroups/\"+classes[i]+\"/\"+data[classes[i]][j]\n",
    "        \n",
    "        ## opening the file\n",
    "        text = open(path, 'r', errors='ignore').read()\n",
    "        \n",
    "        ## going through every word in the file\n",
    "        for word in text.split():\n",
    "            word=word.lower() ## converting the word to lowercase\n",
    "            \n",
    "            ## making sure word is not a stopword and not in the list of our block words\n",
    "            if word not in StopWords and word not in block_words:\n",
    "                \n",
    "                ## claculating the frequency of the word\n",
    "                if word in word_present and word:\n",
    "                    word_present[word]+=1\n",
    "                else:\n",
    "                    word_present[word]=1\n",
    "                    alternate_word_array.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "AV07g-NXNtT7",
    "outputId": "99093092-07fb-43ce-beb4-f2a9ac106a9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425299\n",
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(len(word_present.keys()))\n",
    "print(type(alternate_word_array))\n",
    "print(type(alternate_word_array[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "iyFqPvjENtT_",
    "outputId": "19a83be0-6699-46d4-c70f-b6cd66965ea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189287\n"
     ]
    }
   ],
   "source": [
    "## this piece of code was used to refine the words in our dictonary\n",
    "## that is to remove the special characters present in some words so that we can\n",
    "## work only with pure words.\n",
    "\n",
    "## But using this code gives a lower accuracy so not using.\n",
    "## seems like this ML model doesn't want to work with cleaner data :)\n",
    "\n",
    "x=[]\n",
    "for s in alternate_word_array:\n",
    "    last=0\n",
    "    word_array=[]\n",
    "    j=0\n",
    "    for i in range(len(s)):\n",
    "        if s[i] in punc:\n",
    "            j+=1\n",
    "            if last!=i:\n",
    "                word_array.append(s[last:i])\n",
    "            last = i+1\n",
    "    if last != len(s):\n",
    "        word_array.append(s[last:])\n",
    "    if len(word_array)>=2:\n",
    "        for c in word_array:\n",
    "            if c in word_present:\n",
    "                word_present[c]+=1\n",
    "            else:\n",
    "                word_present[c]=1\n",
    "    if j>0:\n",
    "        x.append(s)\n",
    "        \n",
    "for i in x:\n",
    "    del word_present[i]\n",
    "    \n",
    "print(len(word_present.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "752NM7_VNtUB",
    "outputId": "c8f755dd-2d20-4c4e-dbc2-4bc2fd8c3fca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94899\n"
     ]
    }
   ],
   "source": [
    "## part of the previous cell, used this for cleaning.\n",
    "## not using this so please ignore.\n",
    "\n",
    "x.clear()\n",
    "for s in word_present.keys():\n",
    "    for i in range(len(s)):\n",
    "        if s[i] in num:\n",
    "            x.append(s)\n",
    "            break\n",
    "for i in x:\n",
    "    del word_present[i]\n",
    "print(len(word_present.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "idvCeQ2kNtUE",
    "outputId": "b0254712-616b-4915-b271-0779134a3114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2398\n"
     ]
    }
   ],
   "source": [
    "## using only the words which occur more that 200 times in the 20000 documents\n",
    "\n",
    "x.clear()\n",
    "for s in word_present.keys():\n",
    "    if word_present[s] <= 200:\n",
    "        x.append(s)        \n",
    "\n",
    "## deleting less frequency words from the dictionary\n",
    "\n",
    "for i in x:\n",
    "    del word_present[i]\n",
    "print(len(word_present.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tzPFFYJkNtUH"
   },
   "outputs": [],
   "source": [
    "## making a final list of the words we are using\n",
    "\n",
    "final_words=[]\n",
    "for i in word_present.keys():\n",
    "    final_words.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "iyzfmsiXNtUJ",
    "outputId": "61697173-e2db-44f7-ccc6-fd7889375681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['path:', 'from:', 'mathew', 'newsgroups:', 'subject:', 'alt.atheism', 'atheist', 'resources', 'summary:', 'music', 'related', 'atheism', 'keywords:', 'message-id:', 'date:', 'mon,', '29', 'thu,', 'followup-to:', 'distribution:', 'world', 'approved:', 'lines:', '11', '1992', 'usa', 'freedom', 'religion', 'available', 'us.', 'write', 'to:', 'p.o.', 'box', 'evolution', 'sell', 'ones', 'christians', 'stick', 'word', 'written', '3d', 'north', 'ca', 'people', 'san', 'francisco', 'area', 'mailing', 'net', 'price', 'per', 'american', 'press', 'various', 'books', 'biblical', 'on.', 'book', 'is:', '\"the', 'bible', 'ball', '2nd', 'contains', 'based', 'king', 'james', 'version', 'tx', 'fax:', 'including', '(see', 'east', 'york', 'address', '(which', '59', 'drive,', 'ny', 'organization', 'black', 'history', 'r.', 'americans', 'united', 'national', 'society', 'high', 'street', 'road', 'british', 'south', 'place', '14', 'hall', 'red', 'fax', 'magazine', 'germany', '1.', 'thomas', 'm.', 'santa', 'short', 'proof', 'events', 'living', 'dead', '\"a', 'post', 'spent', 'lives', 'paper', 'white', 'lines', 'example,', 'anyone', 'k.', 'wrote', 'people,', 'truth', 'rather', 'although', 'often', 'believed', 'met', 'sort', 'god,', 'following', 'group', 'earth', 'women', 'remote', 'raise', 'faith', 'joe', 'description', 'christianity', 'reality', 'brain', 'laser', 'possibly', 'friend', 'god', 'making', 'young', 'woman', 'another', 'star', 'system.', 'man', 'whose', 'easy', 'story', 'congress', 'quickly', 'charge', 'nation', 'again.', 'life', 'live', 'christian', 'right', 'property', 'bank', 'radio', 'doctors', 'legal', 'writing', 'difficult', 'first,', 'goes', 'somewhat', 'work', 'however,', 'probably', 'worth', 'know', 'about.', 'exists', 'different', 'true', 'peter', 'de', 'catholic', 'etc.', 'die', 'des', 'michael', 'university', '(i.e.', 'existence', 'belief', 'includes', 'great', 'arguments', 'particular', 'attention', 'paid', 'case', 'best', 'and/or', 'became', 'alternative', 'period', 'considering', 'particularly', 'religious', 'is,', 'single', 'george', 'statements', 'present', '(and', 'opinions', 'people.', 'popular', 'expressed', 'idea', 'quite', 'number', '\"what', 'men', 'think', 'richard', 'second', 'volume', 'began', 'work,', 'attempts', 'series', 'god.', 'upon', 'late', 'western', 'values', 'simple', 'makes', 'attempt', 'j.', 'l.', 'review', 'et', 'moral', 'recent', 'concept', 'beyond', 'read', '-', 'direct', 'compared', 'a.', 'murder', 'looks', 'ancient', 'times', 'day', 'library', 'card', 'above.', 'wide', 'range', 'evil', 'd.', 'mind', 'study', 'become', 'effect', 'them.', \"there's\", 'small', 'archive', 'server', 'old', 'articles', 'files.', 'information,', 'mail', 'saying', 'back', 'file', 'posting', '5', '6', 'april', 'pgp', 'article', 'provide', 'general', 'tried', 'possible', 'regarding', 'always', 'remember', 'document', 'draw', 'relevant', 'listed', 'sense', 'presented', 'questions', 'asked', 'newsgroup', 'frequently', 'note', 'towards', 'faq', 'files', 'actually', 'talk', 'religion,', 'talking', 'discussion', 'apply', 'not.', 'atheists', 'believe', 'former', 'important', 'difference', 'positive', 'fall', 'assuming', 'others', 'limit', 'specific', 'thing', 'definitely', 'means', 'true.', 'something', 'simply', 'whether', 'term', 'professor', 'defined', 'someone', 'things', 'cause', 'thus', 'cannot', 'words', 'things,', 'language', 'point', 'view', 'fact', 'calls', 'mean', 'certainly', 'science', 'basis', 'person', 'ask', 'feel', 'major', 'essentially', 'impossible', 'evidence', 'prove', 'exist', 'larger', 'course,', 'objects', 'matter', 'moment', 'still', 'reasons', 'assume', 'show', 'finding', 'hand', 'question', 'showing', 'require', 'search', 'places', 'there.', 'problem', 'therefore', 'generally', 'accepted', 'unless', 'do.', 'follow', 'rule', 'though', \"can't\", 'default', 'strong', 'usually', 'claim', 'claims', 'cover', 'described', 'close', 'kind', 'universe', 'way,', 'effects', 'way.', 'argue', 'all,', 'easily', 'surely', 'physical', 'caused', 'common', 'game', 'b', 'points', 'true,', 'statement', 'along', 'it,', 'eventually', 'uses', 'original', 'originally', 'needed', 'agree', 'it.', 'seen', 'apparently', 'tend', 'play', 'answer', 'depends', 'meant', 'power', 'especially', '[', 'according', ']', 'definition', 'result', 'human', 'science,', 'watching', '(or', 'act', 'entirely', 'clear', 'in.', 'necessary', 'beliefs', 'data', 'experience', 'laws', 'physics', 'basic', 'ideas', 'called', 'almost', 'everything', 'acts', 'refer', 'certain', 'individual', 'claiming', 'state', 'fit', 'lack', '\"if', 'everyone', 'either', 'position', 'runs', 'speak', 'let', 'mention', 'except', 'perhaps', 'part', 'possible.', 'little', 'outside', '(to', 'soviet', 'church', 'citizens', 'came', 'took', 'control', 'order', 'complete', 'business', 'government', 'individuals', 'concerned', 'remain', 'allow', 'running', \"that's\", 'shall', 'concerning', 'responsible', 'political', 'today,', 'increase', 'foreign', 'long', 'happy', 'care', 'also,', 'told', 'join', 'public', 'friends', 'family', 'reasonable', 'time,', '\"you', 'mentioned', 'object', 'purpose', 'to.', 'well,', 'connection', 'them,', 'obvious', 'excuse', 'building', '\"i', 'strange', 'holding', 'party', \"one's\", 'parents', 'force', 'choose', 'call', 'hand,', 'others.', 'listen', 'heavy', 'prefer', 'carry', 'copy', 'around', 'are,', 'several', 'define', 'morality', 'course', 'within', 'humans', 'social', 'other.', 'enough', 'reason', 'natural', 'happens', 'know,', 'justify', 'full', 'jesus', 'christ', 'save', 'shown', 'display', 'example', 'receive', 'eternal', 'life.', 'quote', 'court', 'serial', 'found', 'behavior', 'done', 'illegal', 'drugs', 'percent', 'sex', 'p.', 'yes,', 'least', 'held', 'no.', 'created', 'seeing', 'rules', 'majority', 'time', 'sometimes', 'considered', 'decision', 'choice', 'nobody', 'figure', 'be.', 'what?', 'merely', 'wants', 'approach', 'decide', 'seriously', 'possibility', 'trying', 'reach', 'open', 'comments', 'there,', 'looking', 'likely', 'wish', 'debate', 'benefit', 'doubt', 'willing', 'basically', 'telling', 'truth,', 'whole', 'completely', 'gives', 'meaning', 'life,', 'hope', 'mark', 'random', 'asking', 'cup', 'is.', 'ways', 'food', 'sound', 'face', 'otherwise', 'consider', 'hard', 'worry', 'years.', 'supposed', 'thousands', 'years', 'wrong', 'stop', 'no,', 'real', 'else.', 'huge', 'money', 'effort', 'imagine', 'died', 'blood', 'countries', 'spread', 'aids', 'known', 'children', 'claimed', 'fight', 'kill', \"who's\", 'maybe', 'support', 'sorry,', 'so,', 'ever', 'pointed', 'start', 'valid', 'hold', 'nothing', 'unfortunately,', 'allows', 'deal', 'that.', 'world,', 'perfectly', 'started', 'explain', 'well.', 'developed', 'quick', 'already', 'out,', 'realize', 'special', 'speaking', 'theory', 'sets', 'convert', 'avoid', 'pressure', 'keep', 'associated', 'disease', 'across', 'fundamental', 'messages', 'surprised', 'external', 'change', 'good.', 'finally', 'question.', 'taking', 'information', 'technical', 'john', 'necessarily', 'q', 'him,', '>', '>this', 'argument', 'me.', 'appear', 'matthew', 'similar', 'knowledge', 'knew', 'obviously', '>i', 'case.', 'date', 'here,', 'age', 'usual', 'says', 'text', 'earlier', 'info', '(i', 'early', 'putting', 'rest', '>>i', 'point,', 'exactly', 'said.', 'older', 'base', 'paul', 'letter', 'it?', 'sure,', 'together', 'id', 'fine.', \"what's\", 'missing', '>and', 'step', 'source', 'learned', 'directly', 'news', 'gets', 'this.', 'generation', 'bad', 'compare', 'reports', '>the', 'versions', '>to', 'break', 'solution', 'material', '9', 'pretty', 'clearly', 'later', '>a', 'end', 'words,', 'connected', 'x-newsreader:', 'ordered', 'none', 'inside', '(at', 'somewhere', 'included', 'next', \"they're\", \"i've\", 'heard', 'defend', '\"', 'university,', 'this?', 'sounds', 'run', 'ibm', 'research', '15', '(bob', 'hate', 'economic', 'worse', 'effective', 'supports', 'gay', 'rights', 'issues', 'somebody', 'rob', '30', 'saw', 'perfect', 'playing', 'terms', 'leads', \">i'm\", 'games', 'here.', 'understand', '>that', 'coming', '>of', 'me,', 'time.', 'shows', 'away', 'ability', 'error', 'place.', 'one.', 'opinion,', '>it', 'happen', 'therefore,', 'greater', 'knowing', 'advance.', 'system', 'good,', 'truly', 'said,', 'appreciate', 'reading', ':)', 'logic', '>there', 'amount', 'fair', 'interested', '(keith', '2', 'california', 'institute', 'technology,', '(ken', '>>the', 'tell', '>with', 'civil', 'required', 'keith', 'islam', '28', 'muslims', 'all.', 'born', 'muslim', 'that,', '>so', 'attack', 'attacks', 'situation', 'count', 'double', 'line', '(jon', '[...]', 'admit', 'yes.', 'switch', 'form', 'goal', 'states', 'system,', 'and,', 'thought', 'killing', '>>and', 'capital', 'objective', 'wrong.', 'current', 'personal', 'wrong,', 'multiple', 'stay', 'becomes', '...', 'exact', '40', 'account', 'changes', '>in', 'population', '12', 'color', '>not', '>have', 'but,', 'judge', 'discuss', 'alt.atheism,talk.religion.misc,talk.origins', '(bill', '(as', 'ad', '>my', 'creation', 'problems.', '25', 'right,', 'right?', 'determine', 'again,', 'too.', 'fully', 'persons', 'parallel', 'poor', 'one,', '44', '(robert', 'continue', 'drive', 'car,', 'later,', 'killed', 'yet', 'anyway.', \"i'm\", \"i'd\", 'car', 'thinks', 'involved', 'gun', 'action', 'prevent', 'options', 'thank', 'getting', 'bit', 'works', 'innocent', 'value', '>you', 'needs', 'risk', 'personally', '50', 'anyway,', 'views', 'local', 'hot', 'recently', 'advantage', 'forced', 'him.', 'entire', 'mean,', 'released', 'watch', 'cost', '>would', 'criminal', 'committed', 'death', 'penalty', 'due', '26', 'absolutely', 'information.', '>if', 'blame', 'member', 'members', 'drives', 'accept', 'traffic', 'driving', 'inc.,', '(james', 'f.', 'higher', 'e', 'indeed', 'rate', 'escape', \"we're\", 'up.', 'bob', 'islamic', '33', 'level', 'sexual', 'request', 'damn', 'right.', 'answers', 'brought', 'large', 'role', '47', 'deaths', 'produce', 'extra', '>--', 'primary', 'practice', 'lost', 'maintain', 'provides', 'why?', 'pin', 'respond', 'thread', '(scott', 'h', 'return', '(was', 'fine', 'purdue', 'engineering', 'computer', 'network', '60', '(john', 'e.', 'numbers', '>as', 'recall', '1', ':', 'totally', 'complex', 'discussions', 'proper', 'tells', 'behind', 'wonder', 'net.', 'type', 'appears', 'facts', 'taken', \"let's\", 'check', 'you?', '(this', 'past', 'chris', 'etc', 'intended', 'calling', 'fact,', 'sign', 'you.', 'worst', 'scott', '***', 'list', '(david', 'talk.religion.misc', 'systems', 'program', 'quality', 'quoted', 'errors', 'given', 'dave', 'false', 'excellent', 'hundred', 'context', 'bunch', 'cut', 'useful', '(jim', 'scientific', 'highly', 'computers', 'create', 'interesting', 'light', 'understanding', 'ok', '(dan', '()', 'dan', '=', '(usenet', 'news)', 'canada', 'folks', '>for', 'do,', 'trouble', 'chance', 'doug', 'fun', '>is', 'actions', 'law', 'criminals', 'felt', 'actual', 'say,', 'opinion', 'include', 'thing,', 'not,', 'case,', 'thinking', '>what', 'serious', 'absolute', 'method', 'comes', 'background', 'then,', '(if', 'degree', 'know.', 'fairly', 'waste', 'justice', 'types', '>on', 'working', 'recognize', 'bear', 'responsibility', 'up,', 'requires', 'instead', 'country', 'on,', 'college', 'deleted]', 'soon', 'postings', 'add', 'nice', '------', 'robert', 'uk', 'email:', 'work.', 'florida', 'international', 'administrator)', 'parts', 'placed', '!', ':-)', 'authority', 'sgi', ')', 'crime', 'ten', 'issue', 'pull', 'us,', 'v.', 'decided', 'groups', 'ok,', 'comment', 'policy', 'file.', 'plan', 'significant', 'so.', '*', '8', 'electronic', 'wait', 'devices', 'computer,', 'inc.', 'march', 'service', 'education', 'male', 'illinois', 'corporation,', 'initial', 'applications', 'contain', 'mission', 'leave', 'condition', 'users', 'serve', 'separate', 'environment', '(1)', 'appropriate', 'areas', '(2)', 'develop', 'among', 'three', 'middle', 'school', 'programs', 'leaders', 'status', 'additional', 'standards', 'teams', 'top', 'functions', 'development', 'copies', 'manual', 'takes', 'home', 'message', 'named', 'board', 'il', 'starting', 'team', 'company,', 'today', 'minority', 'typical', 'technology', 'function', 'format', 'i,', 'space', 'ground', 'says:', 'trust', 'pro', 'conclusion', 'neither', \"i'll\", 'disagree', 'name', 'correct', 'department', 'genocide', '-0400', 'carnegie', 'pittsburgh,', 'pa', '27', 'in-reply-to:', 'side', 'history,', 'notice', 'posts', 'advice', 'peace', 'author', 'email', 'mr.', '31', '(michael', 'gone', 'brian', 'posted', 'knows', '**', 'this,', '(mark', '(the', 'sciences', 'dept.', 'turn', 'process', 'remove', 'government.', 'law,', 'that?', 'i.e.', 'h.', '(news', 'tin', '[version', '1.1', 'pl9]', '34', 'wrote:', '(but', 'nature', ',', 'existing', 'image', 'thanks.', 'communications', '3', 'turkish', 'guy', 'looked', 'blue', 'big', 'stuff.', 'costs', 'news-software:', 'vax/vms', 'vnews', '1.41', '18', 'writes...', '+', 'line.', 'larry', 'guess', 'product', 'suppose', 'now,', 'thing.', 'reduce', 'problem.', 'reserve', 'providing', 'frame', '---', 'love', '(', 'left', 'world.', '43', 'problems', '7', 'days', 'week', 'guide', 'america', 'much.', 'more.', '(stephen', 'host', 'self', 'solid', '#', 'gave', 'holy', 'table', 'reference', '>but', 'examples', 'heaven', 'hell', 'modern', 'passed', 'question:', 'deleted', 'else', 'spirit', 'readers', 'e-mail', '_/', 'express', 'access', 'online', 'md', '}', 'actually,', 'begin', 'aware', 'mine', '(was:', 'texas', 'brother', 'played', 'mother', 'things.', 'thanks,', 'stuff', 'guys', 'four', 'jack', '4', 'manager', 'straight', 'passes', 'summer', 'respect', 'this:', '22', 'account)', 'unix', 'u.', 'denver', 'yeah,', 'now.', 'either.', 'jim', \"god's\", 'forces', '&', 'b.', '46', 'raised', 'code', 'whatever', 'turned', 'out.', 'ago.', '1)', 'learn', '2)', '55', 'fri,', '-0500', '>>>', '3.', 'nuclear', 'military', 'internal', 'war', '(in', 'strongly', 'limited', 'regards,', 'advanced', 'division', 'originator:', 'official', 'move', 'allowed', 'references', 'you,', 'names', 'forward', 'disclaimer:', '|', 'hardly', 'wanted', 'hear', '48', '.', 'question,', 'day,', 'days.', 'missed', 'couple', 'sending', 'jews', '36', 'ken', '>|>', 'previous', 'grant', 'poster', 'jewish', 'nazi', 'active', 'writes', 'job', 'european', 'next?', 'virginia', 'tech', 'va', '42', 'five', 'themselves.', 'stated', 'giving', 'stanford', 'tue,', 'spend', 'paying', 'pay', 't.', 'car.', 'private', 'cars', 'drivers', 'tax', 'design', 'million', '35', 'australia', 'reply', 'changed', '>are', 'greek', 'brand', '(chris', 'pick', 'changing', '13', 'built', 'down,', 'onto', 'german', 'did.', 'bring', 'application', 'c', 'equal', 'vs', 'standard', 'ago', 'suggest', 'share', 'st.', '__', '/', '?', 'morning', 'drawing', 'tom', \"he's\", 'accurate', 'slightly', 'k', 'bbs', 'n', '23', '==', 'system)', 'ignore', 'powerful', 'main', 'character', 'noticed', 'park', 'west', '2.', 'kids', 'provided', 'academic', 'logical', 'year', 'student', 'telephone', 'rich', 'michigan', 'engineering,', \"here's\", 'latest', 'billion', 'mostly', \"we've\", 'sin', 'protection', 'addition,', 'longer', 'to,', 'percentage', 'ago,', 'off.', 'systems,', 'ahead', '*not*', 'ray', '||', 'yet.', '32', '\"we', 'art', 'equipment', 'intelligence', 'oh,', 'clean', 'expect', 'biggest', 'loss', 'site', 'near', 'recommend', 'supported', 'energy', 'fixed', 'half', '2.5', 'replace', 'years,', '(not', 'sorry', 'response', 'forget', 'prior', 'methods', 'summary', 'protect', 'point.', 'details', 'beginning', 'capable', 'game.', '->', '53', 'owner', 'though.', 'night', 'los', '41', 'print', 'operating', 'keyboard', 'land', 'israeli', '51', '>they', 'offered', 'offer', 'however', 'yet,', 'easier', 'apple', 'hours', 'device', 'potential', 'regardless', 'future', 'cdt', 'closed', 'lead', 'properly', 'j', 'g.', 'article-i.d.:', 'nj', '39', 'office', 'homosexual', 'anybody', 'medical', 'president', 'kept', 'austin', 'chicago', 'rid', 'town', '(like', 'atf', 'koresh', 'addition', 'services', 'mary', '>how', 'differences', '??', '4.', '5.', 'x', '6.', '(richard', 'causes', 'measure', 'david', '(mike', 'group.', 'guns', 'u', 'seemed', 'u.s.', '1,', 'safe', 'soldiers', 'vs.', 'machine', 'machines', 'currently', 'failed', 'abortion', 'room', 'minor', 'designed', 'key', 'somehow', '>>in', 'vote', 'amendment', '37', 'piece', 'trial', 'happened', 'round', 'final', 'afford', 'total', 'yes', 'usenet', 'hands', 'digital', 'opportunity', 'though,', '(brian', 'p', 'lab', 'automatic', 'sites', 'company', 'received', 'cases', '(for', 'head', 'memory', 'fear', 'everybody', 'expected', 'game,', '49', 'keeping', '#1', 'voice', '(joseph', 'computing', 'models', 'phone:', 'joseph', 'center', 'studies', 'are.', '(tim', 'sent', 'extremely', 'tim', 'la', '\\\\', 'update', 'relatively', 'alone', 'better.', 'replaced', 'dangerous', '38', 'low', 'stupid', '@', 'uucp:', 'gotten', 'says,', 'seven', 'c.', 'group,', 'third', 'relationship', 'length', 'available.', 'plus', 'defense', 'advance', 'proposed', 'pain', 'proposal', 'levels', 'specifically', 'too,', 'despite', 'water', 'father', 'weeks', 'title', 'child', 'secure', 'homosexuality', '45', 'months', 'etc.)', '>:', 'gateway', 'sit', 'bill', 'fighting', '(paul', '+1', 'sat,', 'away.', 'pass', 'cold', 'law.', 'model', 'ensure', 'extended', 'baseball', '64', 'lived', 'event', 'drop', '80', 'year,', 'bought', '(steve', 'oh', 'opposed', 'results', 'trade', 'van', 'signal', 'media', 'software', 'edt', 'variety', 'entry', 'community', 'article,', 'via', 'interface', 'heart', 'analysis', 'reaction', 'carried', '200', 'letters', 'mode', 'before,', 'btw,', 'historical', 'nt', 'lord', 'operation', 'products', 'images', 'electrical', 'activity', 'weight', 'virtual', 'sun,', 'fast', 'treatment', 'arab', 'health', 'section', 'slow', '2000', 'throughout', 'legitimate', 'covered', '----------------------------------------------------------------------', 'students', 'feeling', 'medicine', 'interest', 'william', 'option', 'appreciated.', 'kinds', 'published', 'conference', 'independent', 'lose', '\"it', 'lower', 'applied', 'english', 'federal', 'win', 'reported', 'daily', 'page', 'ed', 'son', 'house', 'contact', 'hearing', 'government,', 'engine', 'washington', 'record', 'june', 'discovered', 'tool', 'afraid', \"we'll\", 'wings', 'f', 'ice', 'field', 'build', 'hole', 'features', 'secret', 'stopped', 'license', 'mass', 'suggested', 'regular', 'suggestions', '2,', 'anonymous', 'ftp', 'connect', 'directory', \"they've\", 'pages', 'stand', 'sitting', 'activities', 'shoot', 'lots', 'g', 'usenet@news.cso.uiuc.edu', '(net', 'noise', 'owner)', 'urbana', 'mac', 'taxes', 'class', 'clinton', '3rd', 'fbi', 'report', 'user', 'ready', 'picture', '(a', 'note:', 'boston', 'sun', 'at&t', 'na', 'r', 'l', 'wire', 'nearly', 'doctor', 'ii', 'dr.', 'agency', 'jeff', 'sick', 'went', 'frank@d012s658.uucp', '(frank', \"o'dwyer)\", 'talk.abortion,alt.atheism,talk.religion.misc', 'frank', '(andrew', 'attitude', 'front', 'shot', '#>', '(eric', 'n.', '7.', 'six', '$1', '(dave', 'followed', 'communication', 'orbit', 'fan', 'starts', 'bet', 'sports', 'input', 'carrying', 'handle', 'buy', 'gm', 'lets', '#|>', 'regard', 'led', 'troops', 'washington,', 'green', 'canadian', 'police', 'body', 'month', 'wife', 'eric', 'day.', 'flames', 'decent', 'station', 'normally', 's.', '(tom', 'track', 'internet', '100', 'safety', 'toward', 'bodies', 'average', 'tear', 'depending', 'output', 'broken', 'corporation', 'internet:', 'patients', 'mike', '(peter', 'laboratory', 'immediately', 'sources', 'cross', 'weapon', '//', 'added', 'help.', 'cards', '0', 'standing', 'moon', 'army', 'city', 'expensive', 'industry', 'meet', '52', 'wed,', 'vitamin', 'eat', 'programming', '1st', 'israel', 'andrew', 'use.', 'turns', 'compatible', 'flame', 'warrant', '70', 'southern', 'ron', 'california,', 'owners', 'constitution', 'load', 'w.', 'univ.', '-0700', 'privacy', 'hit', 'e-mail:', 'buying', \"they'll\", 'suspect', 'today.', 'solar', 'resolution', 'minnesota', 'selling', '(with', 'obtain', 'chip', 'steve', 'v', 'test', 'gas', 'nec', 'showed', 'moving', 'anywhere', 'convention', 'cleveland,', '(usa)', 'central', 'w', 'hi,', 'hi', ';-)', 'performance', 'store', 'air', '%', 'armed', 'compound', 'bell', 'setting', 'window', 'size', 'smaller', 'oil', 'worked', 'officers', 'video', 'assault', 'weapons', 'alan', 'launch', '3)', 'cs', 'turkey', 'commercial', 'visual', 'command', 'ohio', 'ms', 'moved', 'plain', 'sold', 'arms', 'ram', 'jobs', 'offers', 'toronto', 'pl8]', 'fire', 'nasa', 'flight', '----', 'clipper', 'project', 'documentation', 'cheap', 'shots', 'math', 'speed', 'heat', '___', '(no', 'league', 'europe', 'supply', 'palestinian', 'crypto', 'keys', 'nsa', 'wiretap', 'release', 'sci.crypt', 'branch', 'davidians', 'waco', 'transfer', 'cult', 'tv', 'waiting', 'management', 'throw', 'purchase', 'contact:', 'sales', 'smith', 'program,', 'city,', 'mormons', 'secretary', 'tools', 'drug', 'door', 'market', 'security', 'committee', 'ms.', 'sale', 'export', 'games.', 'algorithm', 'bits', 'distributed', 'link', '..', 'ride', 'riding', 'problem,', 'surface', '(charles', 'greatly', 'soc.religion.christian', 'beat', 'players', 'al', 'fuel', 'client', 'port', 'audio', 'holocaust', 'master', 'dog', '<', 'stats', '$', 'normal', 'phone', 'back.', 'screen', 'feature', 'agents', 'off,', 'hardware', 'enforcement', 'roads', 'turks', 'wondering', 'pat', 'hockey', 'est', 'ford', '1991', 'unit', 'fans', 'circuit', 'compression', 'stanley', 'prices', 'mountain', 'miles', '6,', 'comp.graphics', 'minutes', 'center,', 'pc', '-------------------------------------------------------------------------------', 'colors', 'card.', 'graphics', 'software,', 'sci.med', '256', 'year.', 'windows', 'vga', 'package', 'program.', 'netcom', 'guest)', 'announced', 'companies', 'distribution', 'x11r5', 'windows.', 'printer', 'postscript', 'cd', 'driver', 'fix', 'gif', 'ati', 'edge', 'sweden', 'tape', 'corp.', 'rates', 'dos', 'bios', 'macintosh', 'microsoft', 'jpeg', 'monitor', 'diamond', 'shipping', 'installed', 'tel:', 'faster', 'fonts', 'player', 'upgrade', 'processing', 'resource', 'smith)', '------------------------------------------------------------------------------', 'modem', 'chips', 'leading', 'hp', 'info.', '{', 'mouse', 'file:', 'floppy', 'bus', 'lee', 'rochester', 'cpu', '300', '****', 'select', 'font', 'disk', 'motorola', 'centris', 'boards', 'clock', '/*', '_', 'college,', 'monitors', 'isa', 'meg', 'os/2', 'auto', 'windows,', '3.1', '*/', 'electronics', 'quadra', 'rom', 'motif', 'widget', 'mb', ';', 'block', 'card,', 'satellite', 'disks', 'cheaper', 'agencies', '3.0', 'mit', 'scsi', 'shuttle', 'sci.space', 'install', 'colorado', 'training', '----------------------------------------------------------------------------', 'radar', 'xv', 'annual', 'administration', 'georgia', 'pittsburgh', 'mhz', 'registration', 'louis', 'expansion', 'cable', '(gordon', 'hst', 'gary', 'cancer', 'controller', '(gary', 'msg', 'drive.', 'ran', 'sci.electronics', '$2', 'dc', 'coverage', 'russian', 'ide', 'temperature', 'simms', 'vehicle', 'hd', 'comp.os.ms-windows.misc', 'motherboard', 'db', 'buffalo', 'encryption', 'encrypted', 'dod', 'battery', 'morris', 'insurance', 'dealer', 'xterm', 'sale:', 'bike', 'comp.sys.ibm.pc.hardware', 'season', 'winning', 'comp.sys.mac.hardware', 'rear', 'rocket', 'comp.windows.x', 'hedrick@geneva.rutgers.edu', 'spacecraft', 'honda', 'misc.forsale', 'clutch', 'bmw', 'braves', 'rec.autos', 'batf', 'talk.politics.guns', 'attorney', 'detroit', 'rec.motorcycles', 'lunar', 'rec.sport.baseball', 'espn', 'playoff', 'rec.sport.hockey', 'nhl', 'leafs', '55.0', '(--)', 'escrow', 'firearms', 'talk.politics.misc', 'hedrick@athos.rutgers.edu', 'christian@aramis.rutgers.edu', 'jews?', 'armenian', 'myers:', 'talk.politics.mideast', 'armenia', 'sera@zuma.uucp', '(serdar', 'argic)', 'armenians', 'x-soviet', 'arabs', 'adl', 'stephanopoulos:']\n"
     ]
    }
   ],
   "source": [
    "print(final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SucV4mfsNtUM"
   },
   "outputs": [],
   "source": [
    "## time to convert our dictionary to numpy matrix\n",
    "\n",
    "database = np.zeros((19997,len(final_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t5jTHFs4NtUR"
   },
   "outputs": [],
   "source": [
    "## this function was supposed to accompany the previous data cleaning efforts I made\n",
    "## Like those functions too not using this code\n",
    "\n",
    "def clean(word):\n",
    "    while len(word)>0 and word[-1] in punc:\n",
    "        word = word[:-1]\n",
    "    while len(word)>0 and word[0] in punc:\n",
    "        word = word[1:]\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GeHpySU6NtUT"
   },
   "outputs": [],
   "source": [
    "## Similar story like the previous cell\n",
    "\n",
    "def no_num(s):\n",
    "    for i in range(len(s)):\n",
    "        if s[i] in num:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "RHqtwPIaNtUV",
    "outputId": "17550e82-fdc8-4a97-cdeb-d473d7adfd30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19997\n"
     ]
    }
   ],
   "source": [
    "## this piece of code will again read the words fro every document and help us to\n",
    "## convert our dictionary into a numpy matrix which can be used for multinomialNB\n",
    "\n",
    "counter = 0\n",
    "for i in range(20):\n",
    "    for j in range(len(data[classes[i]])):\n",
    "        \n",
    "        ## declaring the path for every individual file\n",
    "        path = \"./20_newsgroups/\"+classes[i]+\"/\"+data[classes[i]][j]\n",
    "        \n",
    "        ## opening the file\n",
    "        text = open(path, 'r', errors='ignore').read()\n",
    "        \n",
    "        ## going through every word in the file\n",
    "        for word in text.split():\n",
    "            word=word.lower()\n",
    "            \n",
    "            ## making sure word is not a stopword and not in the list of our block words\n",
    "            if word not in StopWords and word not in block_words:\n",
    "                if word in final_words:\n",
    "                    \n",
    "                    ## adding the word frequency to the matrix\n",
    "                    idx = final_words.index(word)\n",
    "                    database[counter][idx] += 1\n",
    "        counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YwBwIS0DNtUY"
   },
   "outputs": [],
   "source": [
    "sum_array = np.sum(database,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "w8uNR6kSNtUa",
    "outputId": "35f2204f-911b-4947-d8e8-7e203516007a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19997,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## this piece of code is used to assign the class to every datapoint in the \n",
    "## database the we created\n",
    "\n",
    "y = []\n",
    "for i in range(len(classes)):\n",
    "    files = os.listdir('./20_newsgroups/' + classes[i])\n",
    "    for j in range(len(files)):\n",
    "        y.append(i)\n",
    "y = np.array(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pA0e6Fz-NtUe"
   },
   "outputs": [],
   "source": [
    "## test train splitting the database\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(database, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "BDSzYS4PNtUf",
    "outputId": "1af5ae1d-8ea0-4b1f-9829-c4cc791b5464"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8728412349136494, 0.81379999999999997)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## applying the multinomialNB from sklearn for our predictions\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "## calculating the training and testing score\n",
    "\n",
    "train_score = clf.score(x_train, y_train)\n",
    "test_score = clf.score(x_test, y_test)\n",
    "\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gY8xJLhpNtUi"
   },
   "outputs": [],
   "source": [
    "f_list = final_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEShIZOfNtUj"
   },
   "outputs": [],
   "source": [
    "## the fit function for our own naive bayes\n",
    "\n",
    "def fit(x_train, y_train):\n",
    "\n",
    "    count={}\n",
    "    set_class = set(y_train)            \n",
    "    for current_class in set_class:\n",
    "        count[current_class] = {}\n",
    "        count[\"total_data\"] = len(y_train)\n",
    "        \n",
    "        ##Rows whose class is current_class\n",
    "        current_class_rows = (y_train == current_class)\n",
    "        \n",
    "        x_train_current = x_train[current_class_rows]\n",
    "        y_train_current = y_train[current_class_rows]\n",
    "        \n",
    "        sums = 0\n",
    "        for i in range(len(f_list)):\n",
    "            ## For each class, calculating total frequency of a feature \n",
    "            count[current_class][f_list[i]] = x_train_current[:,i].sum()\n",
    "            sums = sums + count[current_class][f_list[i]]\n",
    "        \n",
    "        ##Calculating total count of words of a class\n",
    "        count[current_class][\"total_count\"] = sums\n",
    "        \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EiPcRSl8NtUm"
   },
   "outputs": [],
   "source": [
    "def probability(dictionary, row, current_class):\n",
    "    ## class_prob = log of probability of the current class = log(no of documents having class as current_class)/ (total number of documents)\n",
    "    class_prob = np.log(dictionary[current_class][\"total_count\"]) - np.log(dictionary[\"total_data\"])\n",
    "    total_prob = class_prob\n",
    "    \n",
    "    \n",
    "    for i in range(len(row)):\n",
    "        ##Numerator\n",
    "        word_count = dictionary[current_class][f_list[i]] + 1     \n",
    "        ## Denominator\n",
    "        total_count = dictionary[current_class][\"total_count\"] + len(f_list)\n",
    "        ## Add 1 to numerator and len(row) in denominator for laplace correction\n",
    "        \n",
    "        ## Log Probabilty of a word \n",
    "        word_prob = np.log(word_count) - np.log(total_count)\n",
    "        \n",
    "        ##Calculating probability frequency number of times\n",
    "        for j in range(int(row[i])):\n",
    "            total_prob += word_prob\n",
    "        \n",
    "    return total_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLDaYgT1NtUo"
   },
   "outputs": [],
   "source": [
    "def predictSinglePoint(row, dictionary):\n",
    "    classes = dictionary.keys()\n",
    "    \n",
    "    ##Initialising best_prob and best_class as very low count\n",
    "    \n",
    "    best_prob = -1000\n",
    "    best_class = -1\n",
    "    first_iter = True\n",
    "    \n",
    "    for current_class in classes:\n",
    "        if(current_class == \"total_data\"):\n",
    "            continue\n",
    "        \n",
    "        ##Calculating probabilty that the given row belong to current_class\n",
    "        prob_current_class = probability(dictionary, row, current_class)\n",
    "        \n",
    "        ##For first iteration we set the best_prob to be the probabilty that row is of first class and best_class to be first class\n",
    "        ##For rest iteration, we check if the probabilty that row is of the current_class is greater than the best_prob then we update best_prob and best_class.\n",
    "        if(first_iter or prob_current_class > best_prob):\n",
    "            best_prob = prob_current_class\n",
    "            best_class = current_class\n",
    "        \n",
    "        first_iter = False\n",
    "    \n",
    "    ## Return the best class which has maximum probabilty.\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTMvYhstNtUq"
   },
   "outputs": [],
   "source": [
    "def predict(x_test, dictionary):\n",
    "    ## Initialise a list which contain the predictions\n",
    "    y_pred_self = []\n",
    "    \n",
    "    ##Iterate through each row in x_test\n",
    "    for j in range(len(x_test)):\n",
    "        \n",
    "        ##Calculate the prediction of the class to which the row belong to.\n",
    "        pred_class = predictSinglePoint(x_test[j,:], dictionary) \n",
    "        \n",
    "        ##Append the predicted class to our list\n",
    "        y_pred_self.append(pred_class)\n",
    "    \n",
    "    ##Return the list of predictions\n",
    "    return y_pred_self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ih6Fiw9ONtUt"
   },
   "outputs": [],
   "source": [
    "dictionary = fit(x_train, y_train)\n",
    "\n",
    "##Testing the model \n",
    "y_pred_self = predict(x_test, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "C56hnclhNtUu",
    "outputId": "329d6b11-2675-4976-f390-659d233dc5fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for self-implemented Naive Bayes -  0.8166\n",
      "Accuracy for sklearn MultinomialNB() -  0.8138\n"
     ]
    }
   ],
   "source": [
    "## comparing the accuracy of the two models\n",
    "\n",
    "print(\"Accuracy for self-implemented Naive Bayes - \", accuracy_score(y_test, y_pred_self))\n",
    "print(\"Accuracy for sklearn MultinomialNB() - \", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for sklearn MultinomialNB()              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.77      0.74       233\n",
      "          1       0.75      0.72      0.74       253\n",
      "          2       0.76      0.87      0.81       249\n",
      "          3       0.78      0.83      0.80       240\n",
      "          4       0.81      0.87      0.84       236\n",
      "          5       0.89      0.75      0.81       240\n",
      "          6       0.75      0.80      0.78       261\n",
      "          7       0.82      0.91      0.86       269\n",
      "          8       0.80      0.93      0.86       284\n",
      "          9       0.85      0.92      0.88       248\n",
      "         10       0.94      0.84      0.88       231\n",
      "         11       0.96      0.87      0.91       233\n",
      "         12       0.82      0.87      0.85       244\n",
      "         13       0.92      0.85      0.89       256\n",
      "         14       0.87      0.89      0.88       246\n",
      "         15       0.90      0.98      0.94       252\n",
      "         16       0.70      0.82      0.76       249\n",
      "         17       0.92      0.80      0.86       281\n",
      "         18       0.74      0.54      0.62       259\n",
      "         19       0.57      0.42      0.48       236\n",
      "\n",
      "avg / total       0.81      0.81      0.81      5000\n",
      "\n",
      "Classification report for self-implemented Naive Bayes               precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.78      0.74       233\n",
      "          1       0.76      0.73      0.74       253\n",
      "          2       0.77      0.87      0.82       249\n",
      "          3       0.78      0.84      0.81       240\n",
      "          4       0.81      0.87      0.84       236\n",
      "          5       0.89      0.77      0.83       240\n",
      "          6       0.77      0.79      0.78       261\n",
      "          7       0.82      0.91      0.86       269\n",
      "          8       0.81      0.92      0.86       284\n",
      "          9       0.84      0.92      0.88       248\n",
      "         10       0.94      0.84      0.89       231\n",
      "         11       0.96      0.88      0.91       233\n",
      "         12       0.82      0.88      0.85       244\n",
      "         13       0.92      0.85      0.89       256\n",
      "         14       0.87      0.89      0.88       246\n",
      "         15       0.90      0.98      0.94       252\n",
      "         16       0.71      0.83      0.77       249\n",
      "         17       0.92      0.81      0.86       281\n",
      "         18       0.73      0.54      0.62       259\n",
      "         19       0.58      0.42      0.49       236\n",
      "\n",
      "avg / total       0.82      0.82      0.81      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## comparing the classification report of the two algorithms\n",
    "\n",
    "print(\"Classification report for sklearn MultinomialNB()\",classification_report(y_test, y_pred))\n",
    "print(\"Classification report for self-implemented Naive Bayes \",classification_report(y_test, y_pred_self))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TextClassification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
